<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <Title>Projeto 1</Title>
    <link rel="stylesheet" type="text/css" href="css/style2.css">

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@100;200;300;400;500;600;700;800;900&display=swap" rel="stylesheet">

    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" integrity="sha512-iecdLmaskl7CVkqkXNQ/ZH/XLlvWZOJyj7Yy7tcenmpD1ypASozpmT/E0iPtmFIB46ZmdtAc9eNBvH0H/ZpiBw==" crossorigin="anonymous" referrerpolicy="no-referrer" />
</head>
<body>

    <section class="hero">
        <div class="main-width">
            <header>
                <div class="logo">
                    <i class="fa-solid fa-e"></i>

                </div>

                <nav>
                    <div class="hamb">
                        <span></span>
                        <span></span>
                        <span></span>
                    </div>

                   <ul class="nav-list">
                        <li><a href="/index.html">Home</a></li>
                        <li><a href="/sobre.html">Sobre</a></li>
                        <li class="btn"><a href="/Portfolio.html">Portfolio</a></li>
                   </ul> 
                </nav>
            </header>

            <div class="container">
                    <div class="proj-text">
                <!-- * ---------------------------- */  -->
                    <h1><span>Projeto 1 - ETL e Análise de dados com Python</span></h1>
                    <p>Este projeto visa aplicar os conceitos de ETL (Extract, Transform e Load) e análise de dados utilizando python, sobre os dados de crédito do portal IF.data de Conglomerados Financeiros e Instituições Independentes.
                    <br>
                    <br>Perguntas a serem respondidas:
                    <ul>
                    <li>Q1 - Maiores instituições - PF/PJ</li>
                    <li>Q2 - Principais produtos e respectivas taxa de inadimplência - PF/PJ</li>
                    <li>Q3 - Obter a relevância de operação de cartão de crédito para PF</li>
                    <li>Q4 - Qual produto para PJ mais relevante</li>
                    </ul>
                    <br><i>Os dados aqui divulgados são de caracter público e disponíveis no portal IF.data (https://www3.bcb.gov.br/ifdata/).	
                    <br>Sobre o IF.data: Trimestralmente o Banco central divulga informações de instituições em operação e regulamentadas por ele. Dentre as informações divulgadas tem-se a posição de 
                    crédito para pessoa física (PF) e pessoa jurídica (PJ), operações de cambio, informações contábeis e de capital, assim como informações de segmentação.</i>
                    <br>
                    <br>
                    </p>

                    <h2>ETL</h2>
                    <p>Neste projeto a ideia central do processo de ETL é gerar um repositorio (banco de dados) de tabelas relacionais que facilitem analises futuras. O conceito pode 
                        ser entendido como ETL moderno para criação de Data Warehouse, ou seja, visa a criação de um banco com tabelas relacionais e estruturadas a perguntas já definidas.
                    <br>Buscando projetar cada etapa, temos que:
                    <ul>
                    <li>O portal fornece uma API onde podemos coletar os dados, sem ter a necessidade de acessar a página web e fazer o download de arquivos;</li>
                    <li>A tabela obtida contém linhas e colunas que não serão usadas como 'agrupadores (total)' e indices;</li>
                    <li>Os dados obtidos deverão ser armazenados em um banco relacional (no caso, utilizando Postgres).</li>
                    </ul>
                    </p>
                    <br>
                    <br>

                    <h3>Bibliotecas utilizadas</h3>
                    <p>Para as etapas de ETL serão utilizadas as bibliotecas abaixo: </p>
                    <br>
                        <pre style= "background-color: #101010; width: 100%">
                            <code>
        import requests ## responsável pela chamada da API do IFDATA
        import pandas as pd ## responsável pelos tratamentos necessários 
        import psycopg2 ## responsável pela criação e organização do banco de dados e das tabelas
        import psycopg2.extras ## complemento que serve para chamar colunas pelo nome
        from sqlalchemy import create_engine ## responsável por converter o dataframe do pandas para formato sql
                            </code>
                        </pre>
                    <br>
                    <br>

                    <h3>Extract</h3>
                    <p>O ponto central desta etapa é entender a documentação para acesso aos dados, fornecido pelo portal IF.data (https://olinda.bcb.gov.br/olinda/servico/IFDATA/versao/v1/documentacao#IfDataValores) e 
                        aplicar a parametrização correta da biblioteca 'Requests' que foi a utilizada para realizar a comunicação com a API.
                    <br>Relatórios extraídos:
                    <ul>
                    <li>11 - Carteira de crédito ativa Pessoa Física - modalidade e prazo de vencimento </li>
                    <li>13 - Carteira de crédito ativa Pessoa Jurídica - modalidade e prazo de vencimento</li>
                    </ul>
                    <br><i>A extração dos dois relatórios é realizada separadamente, mas com etapas semelhantes.</i>
                    </p>
                    <br>
                    <div>
                    <pre>
                        <code>
        #Extração
        # definição da api-endpoint
        URL = "https://olinda.bcb.gov.br/olinda/servico/IFDATA/versao/v1/odata/IfDataValores(AnoMes=@AnoMes,TipoInstituicao=@TipoInstituicao,Relatorio=@Relatorio)"

        # configurando parametros obrigatórios da api
        PARAMS = {
        '@AnoMes': 202212, #YYYYMM
        '@TipoInstituicao': 2, #Tipo de Instituição: 1 - Conglomerados Prudenciais e Instituições Independentes, 2 - Conglomerados Financeiros e Instituições Independentes, 3 - Instituições Individuais, 4 - Instituições com Operações de Câmbio
        '@Relatorio': "\'11\'", # Carteira de crédito ativa Pessoa Física - modalidade e vencimento
        '$format': 'json'
        }

        # chamando função GET e gravando resultado em um objeto
        r = requests.get(url = URL, params = PARAMS)

        # analisa de o método GET obteve sucesso
        # em caso de erro retorna o código do erro
        if r.status_code == 200:
        print(r.text)
        else:
        print(f"Error: {r.status_code}")
        r.raise_for_status()
                        </code>
                    </pre>
                    </div>
                    <br>
                    <br>

                    <h3>Transform</h3>
                    <p>A partir do json recebido pela etapa anterior (extração), mapeou-se as seguintes necessidades:
                    <ul>
                    <li>Normalizar o json recebido (semi estruturado), retirando a chave 'value' que contem os dados a serem utilizados;</li>
                    <li>A partir da biblioteca pandas, mapear as colunas a serem mantidas;</li>
                    <li>Dropar linhas não utilizadas;</li>
                    <li>Formatar colunas visando padronização;</li>
                    <li>Renomear colunas para a etapa de envio ao banco de dados;</li>
                    </ul>
                    </p>
                    <br>
                    <div>
                    <pre>
                        <code>
        #Tranformação
        # Transforma a resposta da API de text em dicionario, padrão json
        data = r.json()
        # Normaliza o json semi estruturado
        # Extrai apenas o dicionario da chave 'value'
        multiple_level_data = pd.json_normalize(data, record_path =['value'])

        # Monta um dataframe a partir do JSON
        df = pd.DataFrame.from_dict(multiple_level_data)

        # Limpeza e tratamento de colunas
        df2 = pd.DataFrame(df, columns= ['CodInst', 'AnoMes', 'Grupo', 'NomeColuna', 'Saldo'])

        df2['NomeColuna'] = df2['NomeColuna'].str.upper()
        df2['AnoMes'] = pd.to_datetime(df2['AnoMes'], format='%Y%m')
        df2['Saldo'] = df2['Saldo'].round(2)

        # Limpeza das linhas excedentes
        df2 = df2[df2["NomeColuna"].str.contains("TOTAL") == False]

        # Ajuste no nome das colunas para o script levar ao banco de dados
        df2 = df2.rename(columns={'CodInst': 'codinst', 'AnoMes': 'anomes', 'Grupo': 'grupo', 'NomeColuna': 'nomecoluna', 'Saldo': 'saldo'})
                        </code>
                    </pre>
                    </div>
                    <br>
                    <br>

                    <h3>Load</h3>
                    <p>Com os dados limpos e padronizados, podemos avançar e armazena-los dentro de uma estrutura para consultas futuras.
                        Dentre as opções de armazenamento, foi utilizado duas para este projeto:
                        <ul>
                            <li>CSV - arquivo de texto, com valores separados por vírgula. <i>Indicado para facilitar a troca entre ferramentas, volumes pequenos e cenários pontuais</i> </li>
                            <li>SQL - Postgres - software de banco de dados, que permite armazenar dados e criar regras de acesso e modificação. <i>Indicado para cenários rotineiros e que exigem padronização e governança dos dados capturados</i></li>
                        </ul>
                    </p>
                    <br>
                    <div>
                    <pre class="mycode">
                        <code>
        #Armazenamento
        # Para armazenamento em csv - local
        df2.to_csv('dados/IFDATA_PF_db.csv', sep = ';')

        # Para levar informações ao postgreSQL - local

        hostname = 'localhost'
        database = 'dev_lucas'
        username = 'postgres'
        pwd = 'admin'
        port_id = 5432
        conn = None
        cur = None

        engine = create_engine('postgresql+psycopg2://' + username + ':' + pwd + '@' + hostname + '/' + database)

        # Criar tabela se ela não existir no db
        try:
        conn = psycopg2.connect(
        host = hostname,
        dbname = database,
        user = username,
        password = pwd,
        port = port_id
        )

        cur = conn.cursor()

        create_script = ''' CREATE TABLE IF NOT EXISTS ifdata_pf (
                        CodInst         VARCHAR NOT NULL,
                        AnoMes          date NOT NULL,
                        Grupo           VARCHAR NOT NULL,
                        NomeColuna      VARCHAR NOT NULL,
                        Saldo           float  ) '''
        cur.execute(create_script)
        cur.close()

        conn.commit()

        except Exception as error:
        print(error)

        finally:
        if cur is not None:
        cur.close()
        if conn is not None:
        conn.close()

        # Inserir os dados dentro da tabela criada
        df2.to_sql('ifdata_pf', engine, if_exists='append', index=False)
                        </code>
                    </pre>
                    </div>
                    <br>
                    <br>

                    <h2>Análise</h2>
                    <p>Para a análise dos dados foi utilizado um notebook virtual - Jupyter (para facilitar a visualização das etapas), com as bibliotecas pandas e numpy.
                    <br>A fim de explorar ainda mais conceitos a análise de pessoa física partiu do formato csv do ETL e a análise de pessoa jurídica da tabela sql.
                    <br>	
                    </p>
                    <br>

                    <h3>Análise PF</h3>
                    <br>
                    <img src="../img/analise_pf_1.jpg" alt="" class="zoomE" style="max-width: 100%">
                    <img src="../img/analise_pf_2.jpg" alt="" class="zoomE" style="max-width: 100%">
                    <img src="../img/analise_pf_3.jpg" alt="" class="zoomE" style="max-width: 100%">
                    <img src="../img/analise_pf_4.jpg" alt="" class="zoomE" style="max-width: 100%">
                    <br>
                    <br>

                    <h3>Análise PJ</h3>
                    <br>
                    <img src="../img/analise_pj_1.jpg" alt="" class="zoomE" style="max-width: 100%">
                    <img src="../img/analise_pj_2.jpg" alt="" class="zoomE" style="max-width: 100%">
                    <img src="../img/analise_pj_3.jpg" alt="" class="zoomE" style="max-width: 100%">
                    <br>
                    <br>

                    <h2>Conclusão</h2>
                    <br>
                    <h3>Resultados</h3>
                    <p>As respostas para as perguntas tomam com base seus respectivos dataframes gerados na etapa de análise. Assim, 
                        para o período analisado (dados de 12/2022), temos:
                        <br>
                        <br>
                        <img src="../img/resposta_pf.jpg" alt="" class="zoomE" style="max-width: 100%">
                        <img src="../img/resposta_pj.jpg" alt="" class="zoomE" style="max-width: 100%">
                    </p>
                    <br>
                    <h3>Considerações</h3>
                    <p>O projeto teve como foco a aplicação de técnicas de engenharia de dados (ETL) e técnicas de análise de dados (visualização de dados com ipynb).
                        <br> A etapa que exigiu maior atenção foi a extração dos dados da API do portal IF.data, devido as variáveis requisitadas para a obtenção dos dados e a leitura 
                        do json recebido. 
                        <br> Avaliando pontos de melhorias futuras, temos:
                        <ul>
                        <li>Mapear o relatório responsável pelo nome das instituições para facilitar análises;</li>
                        <li>Levar análise de dados para um painel (BI), expandindo a análise para a série histórica;</li>
                        <li>Criar um pipeline dos dados para automatizar o processo de ETL, conforme divulgação dos resultados (trimestral), utilizando Airflow e refinando os scripts em python.</li>							
                        </ul>
                    </p>

                </div>
                <!-- * ---------------------------- */  -->
                <div class="bottom">
                    <p>@ 2023 Lucas - All Rights Reserved.</p>
                </div>
            </div>
        </div>

    </section>

    <script type="text/javascript" src="js/script.js"></script>

</body>
</html>